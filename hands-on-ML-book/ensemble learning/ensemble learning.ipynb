{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mmake_moons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Make two interleaving half circles\n",
       "\n",
       "A simple toy dataset to visualize clustering and classification\n",
       "algorithms. Read more in the :ref:`User Guide <sample_generators>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_samples : int, optional (default=100)\n",
       "    The total number of points generated.\n",
       "\n",
       "shuffle : bool, optional (default=True)\n",
       "    Whether to shuffle the samples.\n",
       "\n",
       "noise : double or None (default=None)\n",
       "    Standard deviation of Gaussian noise added to the data.\n",
       "\n",
       "random_state : int, RandomState instance or None (default)\n",
       "    Determines random number generation for dataset shuffling and noise.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "X : array of shape [n_samples, 2]\n",
       "    The generated samples.\n",
       "\n",
       "y : array of shape [n_samples]\n",
       "    The integer labels (0 or 1) for class membership of each sample.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\malik\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\sklearn\\datasets\\_samples_generator.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_moons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\",random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('log_linear', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log_linear',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=N...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=42,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Method Voting classifier in clearly performing much better then other individual algorithms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft voting \n",
    "\n",
    "voting='soft' in  voting classifier instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     crit...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=42,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm! Working better then hard voting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bagging Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we check the accuracy of individual decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train,y_train)\n",
    "y_pred = tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, can clearly see bagging is better then individual Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.25, 1.5 , 1.75, 2.  ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return a contiguous flattened array.\n",
       "\n",
       "A 1-D array, containing the elements of the input, is returned.  A copy is\n",
       "made only if needed.\n",
       "\n",
       "As of NumPy 1.10, the returned array will have the same type as the input\n",
       "array. (for example, a masked array will be returned for a masked array\n",
       "input)\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like\n",
       "    Input array.  The elements in `a` are read in the order specified by\n",
       "    `order`, and packed as a 1-D array.\n",
       "order : {'C','F', 'A', 'K'}, optional\n",
       "\n",
       "    The elements of `a` are read using this index order. 'C' means\n",
       "    to index the elements in row-major, C-style order,\n",
       "    with the last axis index changing fastest, back to the first\n",
       "    axis index changing slowest.  'F' means to index the elements\n",
       "    in column-major, Fortran-style order, with the\n",
       "    first index changing fastest, and the last index changing\n",
       "    slowest. Note that the 'C' and 'F' options take no account of\n",
       "    the memory layout of the underlying array, and only refer to\n",
       "    the order of axis indexing.  'A' means to read the elements in\n",
       "    Fortran-like index order if `a` is Fortran *contiguous* in\n",
       "    memory, C-like order otherwise.  'K' means to read the\n",
       "    elements in the order they occur in memory, except for\n",
       "    reversing the data when strides are negative.  By default, 'C'\n",
       "    index order is used.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y : array_like\n",
       "    y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n",
       "    Note that matrices are special cased for backward compatibility, if `a`\n",
       "    is a matrix, then y is a 1-D ndarray.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ndarray.flat : 1-D iterator over an array.\n",
       "ndarray.flatten : 1-D array copy of the elements of an array\n",
       "                  in row-major order.\n",
       "ndarray.reshape : Change the shape of an array without changing its data.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In row-major, C-style order, in two dimensions, the row index\n",
       "varies the slowest, and the column index the quickest.  This can\n",
       "be generalized to multiple dimensions, where row-major order\n",
       "implies that the index along the first axis varies slowest, and\n",
       "the index along the last quickest.  The opposite holds for\n",
       "column-major, Fortran-style index ordering.\n",
       "\n",
       "When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n",
       "may be preferable.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "It is equivalent to ``reshape(-1, order=order)``.\n",
       "\n",
       ">>> x = np.array([[1, 2, 3], [4, 5, 6]])\n",
       ">>> np.ravel(x)\n",
       "array([1, 2, 3, 4, 5, 6])\n",
       "\n",
       ">>> x.reshape(-1)\n",
       "array([1, 2, 3, 4, 5, 6])\n",
       "\n",
       ">>> np.ravel(x, order='F')\n",
       "array([1, 4, 2, 5, 3, 6])\n",
       "\n",
       "When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n",
       "\n",
       ">>> np.ravel(x.T)\n",
       "array([1, 4, 2, 5, 3, 6])\n",
       ">>> np.ravel(x.T, order='A')\n",
       "array([1, 2, 3, 4, 5, 6])\n",
       "\n",
       "When ``order`` is 'K', it will preserve orderings that are neither 'C'\n",
       "nor 'F', but won't reverse axes:\n",
       "\n",
       ">>> a = np.arange(3)[::-1]; a\n",
       "array([2, 1, 0])\n",
       ">>> a.ravel(order='C')\n",
       "array([2, 1, 0])\n",
       ">>> a.ravel(order='K')\n",
       "array([2, 1, 0])\n",
       "\n",
       ">>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n",
       "array([[[ 0,  2,  4],\n",
       "        [ 1,  3,  5]],\n",
       "       [[ 6,  8, 10],\n",
       "        [ 7,  9, 11]]])\n",
       ">>> a.ravel(order='C')\n",
       "array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n",
       ">>> a.ravel(order='K')\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\malik\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ravel()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
