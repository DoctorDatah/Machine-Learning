{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(100,3)\n",
    "X.shape # 3d Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40783159, 0.29978538])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10038079, 0.07378706])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 3D data set \n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77830975, 0.1351726 , 0.01034272])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77830975, 0.91348235, 0.92382507])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_)\n",
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(cumsum >= 0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dimension \n",
    "d = np.argmax(cumsum >= 0.95)  + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 784)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07155316, 0.06170876, 0.05401742, 0.04905855, 0.0430278 ,\n",
       "       0.03278245, 0.02884629, 0.02748578, 0.02356632])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09719832, 0.16875148, 0.23046024, 0.28447767, 0.33353621,\n",
       "       0.37656401, 0.40934646, 0.43819275, 0.46567853, 0.48924485,\n",
       "       0.51032629, 0.5307285 , 0.54778858, 0.56465048, 0.58041792,\n",
       "       0.59534958, 0.60862878, 0.62147783, 0.63334578, 0.64479193,\n",
       "       0.65545804, 0.66555448, 0.67514241, 0.68416896, 0.69296211,\n",
       "       0.70131513, 0.70939893, 0.71727437, 0.72468736, 0.73157212,\n",
       "       0.73812949, 0.74459959, 0.75058197, 0.75643475, 0.76210811,\n",
       "       0.7675608 , 0.77261474, 0.77750626, 0.78230885, 0.78696883,\n",
       "       0.79152081, 0.79597374, 0.80014325, 0.80411726, 0.80795962,\n",
       "       0.81171266, 0.81533146, 0.81882001, 0.8221978 , 0.82541301,\n",
       "       0.82859763, 0.83168676, 0.83465474, 0.83752128, 0.84034773,\n",
       "       0.84303834, 0.84571815, 0.84828932, 0.85083357, 0.85329543,\n",
       "       0.85569464, 0.8580644 , 0.86035289, 0.86256245, 0.86468703,\n",
       "       0.86673787, 0.86875875, 0.87071732, 0.87263867, 0.87451472,\n",
       "       0.87637902, 0.87817437, 0.87993644, 0.88167564, 0.88332694,\n",
       "       0.88496271, 0.88657643, 0.88811577, 0.88959381, 0.89101268,\n",
       "       0.89242922, 0.89382729, 0.89522047, 0.8965766 , 0.89790212,\n",
       "       0.89922092, 0.90051436, 0.90177067, 0.90299961, 0.90420227,\n",
       "       0.90536827, 0.90651236, 0.90763806, 0.90874186, 0.90983388,\n",
       "       0.9109034 , 0.91194418, 0.91297678, 0.91397927, 0.91497747,\n",
       "       0.9159598 , 0.91690149, 0.91783568, 0.91874693, 0.9196482 ,\n",
       "       0.92054264, 0.92140428, 0.92225451, 0.92309577, 0.92391214,\n",
       "       0.92470134, 0.92547689, 0.92625005, 0.92701393, 0.92777534,\n",
       "       0.92851944, 0.92925313, 0.92997872, 0.93069175, 0.93139362,\n",
       "       0.93208928, 0.9327773 , 0.93345445, 0.93412851, 0.9347898 ,\n",
       "       0.93543203, 0.93606826, 0.93669991, 0.93731862, 0.93792105,\n",
       "       0.93852032, 0.93911772, 0.93970653, 0.94028677, 0.94086469,\n",
       "       0.94143457, 0.94199662, 0.94254513, 0.94308037, 0.94360648,\n",
       "       0.94412851, 0.94463613, 0.94513923, 0.94563826, 0.94613531,\n",
       "       0.94662917, 0.9471147 , 0.94759725, 0.9480685 , 0.94853659,\n",
       "       0.94900033, 0.94946166, 0.94991842, 0.95036844, 0.95081565,\n",
       "       0.95125944, 0.95169671, 0.95212409, 0.95254411, 0.95296244,\n",
       "       0.9533741 , 0.95378237, 0.95418269, 0.95457805, 0.95496992,\n",
       "       0.9553597 , 0.95574301, 0.95612161, 0.9564982 , 0.95687068,\n",
       "       0.95723707, 0.95760027, 0.95796286, 0.95832109, 0.95867489,\n",
       "       0.95902729, 0.95937418, 0.95971865, 0.96006129, 0.96039856,\n",
       "       0.96073368, 0.96106288, 0.96138943, 0.96171387, 0.96203501,\n",
       "       0.96235523, 0.96267089, 0.96298439, 0.96329457, 0.96360151,\n",
       "       0.96390714, 0.96421066, 0.96451167, 0.96481186, 0.96510904,\n",
       "       0.96540335, 0.96569561, 0.96598681, 0.96627457, 0.96655928,\n",
       "       0.96684207, 0.96712034, 0.96739511, 0.96766553, 0.96793427,\n",
       "       0.96820115, 0.96846617, 0.96872934, 0.96899048, 0.96924982,\n",
       "       0.969508  , 0.9697641 , 0.9700187 , 0.97027163, 0.97052358,\n",
       "       0.97077293, 0.97102026, 0.97126555, 0.9715084 , 0.97175004,\n",
       "       0.97199051, 0.97222924, 0.97246637, 0.97270104, 0.97293287,\n",
       "       0.97316365, 0.97339438, 0.97362278, 0.9738499 , 0.97407402,\n",
       "       0.97429539, 0.97451557, 0.97473531, 0.97495241, 0.97516712,\n",
       "       0.9753797 , 0.97559206, 0.9758017 , 0.97601081, 0.97621875,\n",
       "       0.97642424, 0.97662683, 0.9768281 , 0.97702854, 0.9772268 ,\n",
       "       0.97742402, 0.97761958, 0.97781336, 0.97800674, 0.97819873,\n",
       "       0.97838955, 0.97857832, 0.97876568, 0.97895226, 0.97913858,\n",
       "       0.97932348, 0.97950696, 0.97968875, 0.97987028, 0.98005059,\n",
       "       0.98022851, 0.98040472, 0.98057947, 0.98075307, 0.9809259 ,\n",
       "       0.98109819, 0.98126892, 0.98143933, 0.9816082 , 0.98177628,\n",
       "       0.9819434 , 0.98210892, 0.98227286, 0.98243546, 0.98259722,\n",
       "       0.98275827, 0.98291864, 0.98307788, 0.98323709, 0.98339502,\n",
       "       0.98355127, 0.98370638, 0.98386028, 0.98401377, 0.98416508,\n",
       "       0.984315  , 0.98446428, 0.98461175, 0.98475905, 0.9849048 ,\n",
       "       0.98504911, 0.98519183, 0.98533399, 0.98547554, 0.98561655,\n",
       "       0.98575678, 0.98589473, 0.98603236, 0.98616894, 0.98630493,\n",
       "       0.98643978, 0.98657362, 0.98670677, 0.98683906, 0.98696962,\n",
       "       0.98709868, 0.98722672, 0.98735431, 0.98748084, 0.98760728,\n",
       "       0.98773316, 0.98785848, 0.98798297, 0.9881065 , 0.98822836,\n",
       "       0.98834968, 0.98846973, 0.9885889 , 0.98870762, 0.98882572,\n",
       "       0.98894243, 0.98905877, 0.98917495, 0.98929021, 0.98940457,\n",
       "       0.98951846, 0.98963079, 0.98974208, 0.98985258, 0.98996229,\n",
       "       0.99007131, 0.99018007, 0.9902879 , 0.99039489, 0.99050134,\n",
       "       0.99060741, 0.99071345, 0.99081818, 0.99092154, 0.99102383,\n",
       "       0.99112502, 0.9912259 , 0.99132595, 0.99142524, 0.99152447,\n",
       "       0.99162273, 0.99172049, 0.99181732, 0.99191256, 0.99200678,\n",
       "       0.99210082, 0.99219387, 0.9922857 , 0.99237699, 0.99246821,\n",
       "       0.99255816, 0.99264772, 0.99273682, 0.9928251 , 0.99291217,\n",
       "       0.99299887, 0.99308492, 0.99317032, 0.99325523, 0.99333955,\n",
       "       0.99342333, 0.99350604, 0.99358839, 0.99367043, 0.99375151,\n",
       "       0.99383202, 0.99391198, 0.99399116, 0.9940692 , 0.99414684,\n",
       "       0.99422404, 0.99430025, 0.99437565, 0.99445022, 0.99452425,\n",
       "       0.99459715, 0.99466955, 0.99474154, 0.99481294, 0.99488378,\n",
       "       0.99495376, 0.9950232 , 0.99509214, 0.99515953, 0.99522681,\n",
       "       0.99529301, 0.99535842, 0.99542312, 0.99548711, 0.99555032,\n",
       "       0.99561299, 0.99567446, 0.99573557, 0.99579554, 0.99585539,\n",
       "       0.99591429, 0.99597272, 0.99603058, 0.99608815, 0.99614453,\n",
       "       0.99620069, 0.99625618, 0.99631098, 0.99636529, 0.99641929,\n",
       "       0.99647212, 0.9965248 , 0.99657712, 0.9966289 , 0.99668024,\n",
       "       0.99673134, 0.99678131, 0.99683026, 0.99687909, 0.9969273 ,\n",
       "       0.99697489, 0.99702183, 0.99706829, 0.99711436, 0.99715952,\n",
       "       0.99720397, 0.99724733, 0.9972896 , 0.99733172, 0.99737343,\n",
       "       0.99741513, 0.9974564 , 0.99749694, 0.99753724, 0.99757726,\n",
       "       0.99761601, 0.99765422, 0.99769228, 0.99772964, 0.99776646,\n",
       "       0.99780309, 0.99783946, 0.99787492, 0.99791016, 0.99794503,\n",
       "       0.99797957, 0.99801388, 0.99804796, 0.99808056, 0.99811297,\n",
       "       0.9981451 , 0.99817683, 0.9982082 , 0.99823919, 0.99826903,\n",
       "       0.99829883, 0.9983281 , 0.99835714, 0.99838613, 0.99841416,\n",
       "       0.99844208, 0.99846954, 0.99849649, 0.99852313, 0.99854878,\n",
       "       0.99857388, 0.99859886, 0.99862357, 0.99864806, 0.99867243,\n",
       "       0.99869648, 0.99872032, 0.99874404, 0.99876744, 0.99879048,\n",
       "       0.99881267, 0.9988345 , 0.99885608, 0.99887749, 0.99889864,\n",
       "       0.99891916, 0.99893952, 0.99895978, 0.99897981, 0.99899946,\n",
       "       0.99901888, 0.9990381 , 0.99905721, 0.99907557, 0.9990937 ,\n",
       "       0.99911153, 0.99912922, 0.99914654, 0.99916358, 0.99917999,\n",
       "       0.99919613, 0.99921205, 0.99922778, 0.99924315, 0.9992583 ,\n",
       "       0.99927306, 0.99928772, 0.99930234, 0.99931682, 0.99933089,\n",
       "       0.99934469, 0.99935833, 0.99937185, 0.99938527, 0.99939863,\n",
       "       0.99941127, 0.99942374, 0.99943595, 0.99944804, 0.99946005,\n",
       "       0.99947194, 0.99948361, 0.99949519, 0.99950668, 0.99951806,\n",
       "       0.99952915, 0.99954015, 0.99955084, 0.99956134, 0.99957163,\n",
       "       0.99958178, 0.99959178, 0.99960108, 0.99961025, 0.99961928,\n",
       "       0.99962827, 0.9996371 , 0.99964575, 0.99965396, 0.99966212,\n",
       "       0.9996701 , 0.99967789, 0.9996856 , 0.99969319, 0.99970074,\n",
       "       0.99970822, 0.99971566, 0.99972297, 0.99973022, 0.99973738,\n",
       "       0.99974449, 0.99975131, 0.99975796, 0.99976453, 0.99977098,\n",
       "       0.99977683, 0.99978254, 0.99978813, 0.9997937 , 0.99979917,\n",
       "       0.99980458, 0.9998099 , 0.99981519, 0.99982026, 0.9998251 ,\n",
       "       0.99982993, 0.99983476, 0.99983935, 0.99984388, 0.99984835,\n",
       "       0.99985278, 0.99985699, 0.99986112, 0.99986511, 0.9998691 ,\n",
       "       0.99987306, 0.99987678, 0.99988033, 0.99988386, 0.99988722,\n",
       "       0.99989054, 0.99989383, 0.99989712, 0.99990034, 0.99990348,\n",
       "       0.99990655, 0.99990948, 0.99991231, 0.99991507, 0.99991781,\n",
       "       0.99992045, 0.99992286, 0.99992525, 0.99992763, 0.99992989,\n",
       "       0.99993209, 0.99993424, 0.99993637, 0.99993848, 0.9999405 ,\n",
       "       0.99994245, 0.99994433, 0.9999462 , 0.99994801, 0.99994981,\n",
       "       0.99995156, 0.9999533 , 0.99995494, 0.99995657, 0.99995819,\n",
       "       0.99995978, 0.9999613 , 0.99996263, 0.99996394, 0.99996523,\n",
       "       0.9999665 , 0.99996776, 0.99996896, 0.99997012, 0.99997126,\n",
       "       0.99997237, 0.99997345, 0.99997451, 0.99997555, 0.99997653,\n",
       "       0.99997748, 0.99997841, 0.99997932, 0.99998018, 0.99998103,\n",
       "       0.99998185, 0.99998267, 0.99998347, 0.99998422, 0.99998496,\n",
       "       0.99998567, 0.99998636, 0.99998703, 0.9999877 , 0.99998832,\n",
       "       0.99998892, 0.99998947, 0.99999   , 0.99999046, 0.99999092,\n",
       "       0.99999138, 0.99999182, 0.99999222, 0.9999926 , 0.99999299,\n",
       "       0.99999335, 0.99999371, 0.99999405, 0.99999438, 0.99999471,\n",
       "       0.99999503, 0.99999533, 0.99999562, 0.99999588, 0.99999613,\n",
       "       0.99999637, 0.99999659, 0.99999682, 0.99999704, 0.99999725,\n",
       "       0.99999746, 0.99999766, 0.99999785, 0.999998  , 0.99999816,\n",
       "       0.9999983 , 0.99999843, 0.99999856, 0.99999867, 0.99999878,\n",
       "       0.99999889, 0.99999899, 0.99999909, 0.99999918, 0.99999926,\n",
       "       0.99999935, 0.99999943, 0.99999949, 0.99999954, 0.9999996 ,\n",
       "       0.99999965, 0.99999969, 0.99999972, 0.99999976, 0.99999979,\n",
       "       0.99999981, 0.99999983, 0.99999985, 0.99999986, 0.99999988,\n",
       "       0.9999999 , 0.99999991, 0.99999993, 0.99999994, 0.99999995,\n",
       "       0.99999996, 0.99999997, 0.99999997, 0.99999998, 0.99999998,\n",
       "       0.99999999, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum >= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([False,False,False, False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([False,False,False, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([False,False,True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([True,True,True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([False,False,False, False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([False,True,False, True, False, True])\n",
    "\n",
    "# For getting dim we add 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(cumsum >= 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca  = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 154)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
